{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import pickle\n",
                "import numpy as np\n",
                "import random\n",
                "import os\n",
                "from tqdm import tqdm\n",
                "\n",
                "import torch\n",
                "from torch import nn, optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.tensorboard import SummaryWriter\n",
                "\n",
                "from transformers import GPT2LMHeadModel\n",
                "from transformers import Trainer, TrainingArguments\n",
                "from transformers import GPT2Tokenizer\n",
                "\n",
                "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# By order of size:\n",
                "# gpt-2\n",
                "# gpt-2-medium\n",
                "# gpt-2-large\n",
                "# gpt-2-xl\n",
                "\n",
                "MODEL_VERSION = \"gpt2\"\n",
                "\n",
                "model = GPT2LMHeadModel.from_pretrained(MODEL_VERSION)\n",
                "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_VERSION)\n",
                "model.to(DEVICE)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "class WayneDataset(Dataset):\n",
                "    def __init__(self, data_src, sequence_length):\n",
                "        self.sequence_length = sequence_length\n",
                "\n",
                "        with open(data_src, \"rb\") as f:\n",
                "            self.original_data = pickle.load(f)\n",
                "\n",
                "        self.shuffle()\n",
                "\n",
                "    def shuffle(self):\n",
                "        # Shift the dataset every time we shuffle, so the sequences aren't always cut in the same place\n",
                "        offset = random.randrange(0, self.sequence_length)\n",
                "        self.data = self.original_data[offset:] + self.original_data[:offset]\n",
                "\n",
                "        # Pad in the end\n",
                "        self.data = self.data + (\n",
                "            [tokenizer.eos_token_id] * (self.sequence_length - (len(self.data) % self.sequence_length))\n",
                "        )\n",
                "\n",
                "        # Cut into sequences with length = self.sequence_length\n",
                "        self.data = np.array(self.data).reshape(-1, self.sequence_length)\n",
                "\n",
                "    def __len__(self):\n",
                "        return self.data.shape[0]\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return torch.from_numpy(self.data[idx]).long()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Writer will output to ./runs/ directory by default\n",
                "writer = SummaryWriter()\n",
                "\n",
                "BATCH_SIZE = 16\n",
                "\n",
                "train_dataset = WayneDataset(\"../data/dataset/lil-wayne-train.pkl\", 120)\n",
                "test_dataset = WayneDataset(\"../data/dataset/lil-wayne-test.pkl\", 120)\n",
                "\n",
                "loss_fn = nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.0)\n",
                "\n",
                "def evaluate(model):\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        generation_output = model.generate(\n",
                "            torch.tensor([[tokenizer.eos_token_id]]).to(DEVICE),\n",
                "            return_dict_in_generate=True,\n",
                "            output_scores=True\n",
                "        )\n",
                "        test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
                "\n",
                "        total_loss = 0\n",
                "\n",
                "        for i, x in tqdm(enumerate(test_dataloader), total=test_dataset.__len__() // BATCH_SIZE):\n",
                "            x = x.to(DEVICE)\n",
                "            y = model.forward(input_ids=x).logits\n",
                "\n",
                "            loss = loss_fn(y.view((-1, 50257)), x.view((-1,))) \n",
                "            total_loss += loss.item()\n",
                "\n",
                "        return total_loss, tokenizer.decode(generation_output.sequences[0])\n",
                "\n",
                "\n",
                "batches_per_epoch = train_dataset.__len__() // BATCH_SIZE\n",
                "test_loss, test_generation = evaluate(model)\n",
                "writer.add_scalar(\"Loss/test\", test_loss, 0 * batches_per_epoch)\n",
                "writer.add_text(\"Generation\", test_generation, 0 * batches_per_epoch)\n",
                "\n",
                "for epoch in range(100):\n",
                "    print(\"EPOCH\", epoch)\n",
                "    model.train()\n",
                "    train_dataset.shuffle()\n",
                "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "\n",
                "    for i, y in tqdm(enumerate(train_dataloader), total=batches_per_epoch):\n",
                "        y = y.to(DEVICE)\n",
                "        x = torch.cat([torch.full((BATCH_SIZE, 1), tokenizer.eos_token_id).long().to(DEVICE), y[:, :-1]], dim=1)\n",
                "        y_ = model.forward(input_ids=x).logits\n",
                "        \n",
                "\n",
                "        loss = loss_fn(y_.view((-1, 50257)), y.view((-1,)))\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        optimizer.zero_grad()\n",
                "\n",
                "        writer.add_scalar(\"Loss/train\", loss.item(), epoch * batches_per_epoch + i)\n",
                "    \n",
                "    test_loss, test_generation = evaluate(model)\n",
                "\n",
                "    checkpoint_path = \"./checkpoints/\" + writer.get_logdir()[5:] + \"/epoch-\" + str(epoch)\n",
                "    if not os.path.exists(checkpoint_path):\n",
                "        os.makedirs(checkpoint_path)\n",
                "    model.save_pretrained(checkpoint_path)\n",
                "    \n",
                "    writer.add_scalar(\"Loss/test\", test_loss, (epoch + 1) * batches_per_epoch)\n",
                "    writer.add_text(\"Generation\", test_generation, (epoch + 1) * batches_per_epoch)\n"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit"
        },
        "interpreter": {
            "hash": "c1cc69e61c0f1c7ade8df0f2994e582e7c1f2c57d1ec192a0baf9f96b7739d9d"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}